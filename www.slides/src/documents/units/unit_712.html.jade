---
layout: 'deck_unit'
title: 'Pure Goal Ascription: the Teleological Stance'
tags: []
description: 'Pure goal ascription is goal ascription which occurs independently of any knowledge of mental states.  Here we consider an account of pure goal ascription involving the teleological stance.'
depends: ['']  
source: ['']
duration: 5
book: []
exercises: []
exercises_fast: []
---

include ../../../fragments/unit_mixins

//- (have detailed notes written up in lecture06.notes.pdf in folder 'lecture06 goal ascription teleological motor' from CEU Budapest lectures)
  

+slide_middle
  .notes These two questions are closely related:
  ol.notes.show
    li What model of action underpins  six- or twelve-month-old infants’ abilities to track the goals of actions? 
    li How could infants identify goals without ascribing intentions?
  .notes: :t()
    The first concerns the model of action, the second the process of ascription.
    As we'll see, we can find an answer to the first question by thinking about the second.
    (Compare the physical case: the question of which model of the physical you are adopting,
    e.g. impetus vs Newtonian mechanics, is closely related to the question of how you make 
    predictions about objects' movements and interactions.)
  +highlight-row('li:eq(1)')
  .notes: :t()
    Although the first is out current question, we'll approach it by asking the second.

//- pure goal ascription
+slide_middle
  p.center
    span.hide.step2 How could 
    span.hide.step1.italic pure 
    span goal ascription 
    span.hide.step2 work?
  .notes \newcommand{\dfGoalAscription}{\emph{Goal ascription} is the process of identifying outcomes to which purposive actions are directed as outcomes to which those actions are directed.}
  .notes \dfGoalAscription{}
  +show('.step1')
  .notes Pure goal ascription is goal ascription which occurs independently of any knowledge of mental states.  
  +show('.step2')
  .notes: :t()
     In looking for an alternative model of action we are also asking, How could pure goal 
     ascription work?
     Could you maybe think about the goal and not the intention and that would be enough?
     Well no because a goal is just an outcome.  Let me try to explain with an example ...

//- dropping an egg
+slide_img('slide_budapest_lecture06_019.jpg')
  .notes: :t()
     Earlier I said that \dfGoalAscription{}
     Given this definition, goal ascription involves three things:
     \begin{enumerate}
     \item representing an action
     \item representing an outcome
     \end{enumerate}
     and
     \begin{enumerate}[resume]
     \item capturing the directedness of the action to the outcome.
     \end{enumerate}
     &nbsp;
  .notes: :t()
    It is important to see that the third item---capturing directedness---is necessary.
    This is quite simple but very important, so let me slowly explain why goal ascription requires 
    representing the directedness of an action to an outcome.
    Imagine two people, Ayesha and Beatrice, who each intend to break an egg.
    Acting on her intention, Ayesha breaks her egg.
    But Beatrice accidentally drops her egg while carrying it to the kitchen.
+slide_img('slide_budapest_lecture06_020.jpg')
  .notes: :t()
    So Ayesha and Beatrice perform visually similar actions which result in the same type of outcome, the breaking of an egg; 
    but Beatrice's action is not directed to the outcome of her action whereas Ayesha's is.
+slide_img('slide_budapest_lecture06_021.jpg')
  .notes: :t()
     Goal ascription requires the ability to distinguish between Ayesha's action and Beatrice's 
     action. 
     This requires representing not only actions and outcomes but also the directedness of actions 
     to outcomes.
  .notes: :t()
     This is why I say that goal ascription requires capturing the directedness of an action to an 
     outcome, and not just representing the action and the outcome.

+slide_middle
  .notes: :t()
     So how could pure goal ascription work?
     How could we represent the directedness of an action to an outcome without representing an 
     intention?
  p.center
    span.step2 How could 
    span.step1.italic pure 
    span goal ascription 
    span.step2 work?





//- this slide is part of the slide below
mixin slide_about-teleological-function()
  p aside: what is a teleological function?
  .notes: :t()
    What do we mean by teleological function?
  .slide
    .notes: :t()
      Here is an example:
      %
      \begin{quote}
    p.em-above.notes.show       Atta ants cut leaves in order to fertilize their fungus crops (not to thatch the entrances to their homes) \citep{Schultz:1999ps}
    .notes      \end{quote}
  .slide
    .notes: :t()
      What does it mean to say that the ants’ grass cutting has this goal rather than some other? According to Wright:
      \begin{quote}
    p.em-above.notes.show ‘S does B for the sake of G iff: (i) B tends to bring about G; (ii) B occurs because (i.e. is brought about by the fact that) it tends to bring about G.’ (Wright 1976: 39)
    .notes \end{quote}
  .slide
    .notes: :t()
      For instance:
      %
      \begin{quote}
    p.em-above.notes.show The Atta ant cuts leaves in order to fertilize iff: (i) cutting leaves tends to bring about fertilizing; (ii) cutting leaves occurs because it tends to bring about fertilizing.
    .notes \end{quote}



+slide({id:'big-goal-ascription-slide'})
  .notes: :t()
    To explain the possibility of pure goal ascription we need to find 
    a relation, $R$, such that:
    \begin{enumerate}
    \item reliably, $R(a,G)$ when and only when $a$ is directed\footnotemark to $G$; 
    \item $R(a,G)$ is readily detectable; and 
    \item $R(a,G)$ is readily detectable independently of any knowledge of mental states.
    \end{enumerate}
  .notes \footnotetext{
  .notes.ctd: :t()
    We want this to be true whether $a$’s being directed to $G$ involves intention, function or 
    motor representation.
    }
  .notes: :t()
    We can make progress in explaining how pure goal ascription could work by identifying one or 
    more values of $R$.
    What could $R$ be?
  p.em-above Three requirements ...
  ol.three-requirements(type='i', style="list-style-type: lower-roman")
    li reliably, R(a,G) 
      span.when when
      span  and 
      span.only-when only when
      span  a is directed to G
    li R(a,G) is readily detectable ...
    li ... without any knowledge of mental states
  .notes What could this relation be?
  //- this table lists the various ideas about what the relation R could be
  //- each row is an idea
  table
    tr.idea1.hide
      td R(a,G) 
      td(style='padding-left:.5em;padding-right:.5em;') =
        sub df
      td  a causes G ?
    tr.idea2.hide
      td R(a,G) 
      td(style='padding-left:.5em;padding-right:.5em;') =
        sub df
      td  G is a 
        span.teleological-function teleological function
        span  of a ?
    tr.idea3.hide
      td R(a,G) 
      td(style='padding-left:.5em;padding-right:.5em;') =
        sub df
      td  
        span.qqq ???
        span.remove-me.the-idea a is the most justifiable action towards G available within the constraints of reality
    tr.idea3-reformulated.hide
      td 
      td(style='padding-left:.5em;padding-right:.5em;') =
        sub df
      td (1)-(5) are true 
      
  //- first idea
  +show('.idea1')
  .notes: :t()
    How about taking $R$ to be causation?
    That is, how about defining $R(a,G)$ as $a$ causes $G$?
  +highlight-row('.three-requirements li:eq(1)')
  .notes: :t()
    This proposal does meet the second requirement: causal relations are readily detectable,
    even by infants, as we saw.
  +highlight-row-remove_('.three-requirements li:eq(1)')
  +highlight-row('.three-requirements li:eq(2)')
  .notes: :t()
    This proposal also meets the third requirement: causal relations do not generally require 
    identifying mental states
  +highlight-row-remove_('.three-requirements li:eq(2)')
  +highlight-row('.three-requirements li:eq(0)')
  .notes: :t()
    But this proposal does not meet the first criterion, (1), above.
    (This is the requirement that reliably, R(a,G) when and only when a is directed to G.)
    We can see this by mentioning two problems.
  +highlight('.only-when')  
  .notes: :t()
    First problem: actions typically have side-effects which are not goals.
    For example,
    suppose that I walk over here with the goal of being next to you.
    This action has lots of side-effects: 
    \begin{itemize}
    \item I will be at this location.
    \item I will expend some energy.
    \item I will be further away from the front
    \end{itemize}
    These are all causal consequence of my action.
    But they are not goals to which my action is directed.
    So this version of $R$ will massively over-generate goals.
  +highlight-remove_('.only-when')  
  +highlight('.when')  
  .notes: :t()
    Second problem: actions can fail.  [...]
    So this version of $R$ will under-generate goals.

  //- second idea
  +highlight-remove_('.when')  
  +highlight-row-remove_('.three-requirements li:eq(0)')
  +blur2_('.idea1','3px')
  +show('.idea2')
  .notes: :t()
    Why not define $R$ in terms of teleological function?
  +highlight('.idea2 .teleological-function')
    .notes First, what is teleological function? ...
  +highlight-remove_('.idea2 .teleological-function')
  .slide.about-teleological-function
    +clear-slide
      +slide_about-teleological-function
  +hide('.about-teleological-function *')
  .notes: :t()
    So, to return to the idea, why not define $R$ in terms of teleological function?
  +highlight-row('.three-requirements li:eq(1)')
  .notes: :t()
    I do not think this idea will enable us to meet the second condition.
    How could we tell whether an action happens \emph{because} it brought about a particular 
    outcome in the past? 
    This might be done with insects.
    But it can's so easily be done with primates, who have a much broader repertoire of actions
    and wider range of motivations.
  +highlight-row-remove_('.three-requirements li:eq(1)')
  +highlight-row('.three-requirements li:eq(0)')
  .notes: :t()
    There is a related problem with the first requirement.
    The problem is that we primates can perform old actions in order to achieve novel goals.
    In such cases there will be a mismatch between the goals of our actions and their teleological 
    functions.
    Maybe we should allow that this idea will sometimes enable goal ascription to succeed,
    but it will probably not allow for a very wide range of goals to be correctly ascribed.
    
  //- third idea
  +highlight-row-remove_('.three-requirements li:eq(0)')
  +blur2_('.idea2','3px')
  +show('.idea3')
  .notes: :t()
    So what could R be?  I think we can get a good idea by considering Csibra and Gergely's
    ideas about the teleological stance.
    They are making (in effect) a promising proposal about R.
  .slide
    .teleological-stance-quote
      p ‘an action can be explained by a goal state if, and only if, it is seen as the most justifiable action towards that goal state that is available within the constraints of reality’
      p.right (Csibra & Gergely 1998: 255)
  +hide_('.teleological-stance-quote')
  +remove_('.idea3 .qqq')
  +unremove('.idea3 .the-idea')
  .notes This idea needs further scrutiny ...



+slide_middle
  .notes So here we are discussing what Gergely & Csibra call teleological stance 
  p.center the teleological stance (Gergely & Csibra)


+slide({bkg:'unit_712/slide_conf_ceu_interacting_mindreaders_10.jpg'})
  .notes Csibra and Gergely offer a 'principle of rationality' according to which ...
  .notes.handout Csibra & Gergely's principle of rational action: `an action can be explained by a goal state if, and only if, it is seen as the most justifiable action towards that goal state that is available within the constraints of reality.'\citep{Csibra:1998cx,Csibra:2003jv}
  .handout.notes: :t()
    (Contrast a principle of efficiency:
    `goal attribution requires that agents expend the least possible amount of energy within their 
    motor constraints to achieve a certain end' \citep[p.\ 1061]{Southgate:2008el}).
  .notes &nbsp;  
  .notes This principle plays two distinct roles.
  .notes One role is mechanistic: this principle forms part of an account of how infants (and others) actually ascribe goals.
  .notes Another role is normative: this principle also identifies grounds on which it would be rational to ascribe a goal.
  .notes &nbsp;  
  .notes As Csibra and Gergely formulate it, the principle might seem simple.
  .notes But actually their eloquence is hiding some complexity.

+slide({bkg:'unit_712/slide_conf_ceu_interacting_mindreaders_11.jpg'})
  .notes How are we to understand 'justifiable action towards that goal state?'

+slide({bkg:'unit_712/slide_conf_ceu_interacting_mindreaders_12.jpg'})
  .notes It is perhaps worth spelling out what might be involved in applying this principle.
  .notes Let me try to spell it out as an inference with premises and a conclusion ...
+slide({bkg:'unit_712/slide_conf_ceu_interacting_mindreaders_13.jpg'})
+slide({bkg:'unit_712/slide_conf_ceu_interacting_mindreaders_14.jpg'})
+slide({bkg:'unit_712/slide_conf_ceu_interacting_mindreaders_15.jpg'})
+slide({bkg:'unit_712/slide_conf_ceu_interacting_mindreaders_16.jpg'})
  .notes: :t()
    [*these notes are a bit jumbled ... I'm trying to fix some problems with their view in order
    to focus on a key objection.]
  .notes: :t()
    What do we mean by `better means'?
    Problem with defining $R$ in terms of rationality is requirement of core knowledge / modularity.
    So what about efficiency instead of rationality?
    One problem with defining $R$ in terms of minimising energy is that in 
    acting we often face a trade off between how much energy to put into an action and how likely 
    the action is to result in success.
  .notes: :t()
    Suppose I can save some energy by throwing the cup at the sink instead of walking over and 
    carefully placing it in the sink,
    and suppose that I choose to walk over and place the cup in the sink.
    In this situation the principle of efficiency fails to identify $G$, placing the cup in the 
    sink, as the goal of my action.
  .notes: :t()
    One way to address this problem might be to think of efficiency in terms of achieving a good trade-off between several factors:
    not just energy but also the probability that a particular action will in fact result in the goal being achieved.
    This is the idea I am trying to get at here ...

  .notes An action of type $a'$ is a better means of realising outcome $G$ in a given situation than an action of type $a$ if, for instance, actions of type $a'$ normally involve less effort than actions of type $a$ 
  .notes in situations with the salient features of this situation 
  .notes and everything else is equal; 
  .notes or if, for example, actions of type $a'$ are normally more likely to realise outcome $G$ than actions of type $a$
  .notes in situations with the salient features of this situation 
  .notes and everything else is equal.
  .notes &nbsp;
  .notes A problem with what we have so far is side-effects, which can be highly reliable.
  .notes Actions typically have side-effects which are not goals.  For example,
  .notes suppose that I walk over here with the goal of being next to you.
  .notes This action has lots of side-effects: 
  .notes \begin{itemize}
  .notes \item I will be at this location.
  .notes \item I will expend some energy.
  .notes \item I will be this much further away from the front
  .notes \end{itemize}
  .notes These  are not goals to which my action is directed.
  .notes But they are things which my action would be a rational and efficient way of bring about.
  .notes So there is a risk that these optimising versions of $R$ will over-generate goals.
+slide({bkg:'unit_712/slide_conf_ceu_interacting_mindreaders_17.jpg'})
  .notes I think this first problem can be solved by adding a clause about desire.
  .notes [*] We can substantially mitigate the problem of side-effects by requiring that $R(a,G)$ hold only where $G$ is the type of outcome which is typically desirable for agents like $a$.
+slide({bkg:'unit_712/slide_conf_ceu_interacting_mindreaders_18.jpg'})
+slide({bkg:'unit_712/slide_conf_ceu_interacting_mindreaders_19.jpg'})
  .notes Now so far we've been considering this as an account of how someone could identify to which goal an action is directed without thinking about mental states.
  .notes That is, this inference is the core component in an account of pure goal ascription.
  .notes This gives us, in effect, a specification of R.


section.slide
  .slide-clone(data-which='#big-goal-ascription-slide')
  +show('.idea3-reformulated')
  .notes: :t()
    I've spent some time formulating this idea because I think it's a good candidate.
    We are not yet ready to accept the idea, though.
    Let's consider whether it meets the three requirements.
  +highlight-row('.three-requirements li:eq(2)')
  .notes: :t()
    I take it the third requirement is obviously met
  +highlight-row-remove_('.three-requirements li:eq(2)')
  +highlight-row_('.three-requirements li:eq(0)')
  +highlight-row('.three-requirements li:eq(1)')
  .notes: :t()
    But what about the first and second requirements?
    It's just here that I think there is a problem we need to solve.
  .notes: :t()
    This is the probem.
    How good is the agent at optimising the rationality, or the efficiency, of her actions?
    And how good is the observer at identifying the optimality of actions in relation to outcomes?
    \textbf{
    For the relation to be readily detectible, we want there to be a match between
    (i) how well the agent can optimise her actions
    and
    (i) how well the observer can detect optimality.}
    Failing such a match, the relation R will either not be detectible or not be reliable.

+slide
  .notes: :t()
    Csibra and Gergely seem both aware of this issue and dismissive of it.
  p.handout.notes.show
    span `Such calculations require 
    span.highlight detailed knowledge of biomechanical factors
    span.step2.hide
      span  that determine the motion capabilities and energy expenditure of agents. 
      span However, in the absence of such knowledge, one can appeal to 
      span.highlight heuristics
      span  that approximate the results of these calculations on 
      span the basis of knowledge in other domains that is certainly available to young infants. 
      span For example, the length of pathways can be assessed by 
      span.highlight geometrical calculations
      span , taking also into account some physical factors 
      span (like the impenetrability of solid objects). 
      span Similarly, the fewer steps an action sequence takes, the less effort it might require, 
      span and so 
      span infants’ 
      span.highlight numerical competence
      span  can also contribute to efficiency evaluation.’
  +invert('.highlight')
  +show('.step2')
  p.right Csibra & Gergely (forthcoming ms p. 8)
  .notes: :t()
    Csibra and Gergely seem both aware of this issue and dismissive of it.
    In short their solution to the problem--the problem of matching optimisation in planning actions 
    with optimisation in predicting them--appears to be to insist that infants just do really
    complex detection.  But this threatens the ready detectibility of the relation.

+slide
  .notes Let me offer a quick interim summary.
  p summary so far
  p.slide.em-above How could pure goal ascription work?
  p.slide.em-above R(a,G) = ???
  p.slide.em-above teleological stance
  p.slide.em-above problem: detecting optimality

+slide_middle
  p.center a solution?

+slide
  p.notes.show goal ascription is acting in reverse
  .notes: :t()
    The idea is that we could solve the problem--the problem of matching optimisation in planning 
    actions 
    with optimisation in predicting them--by supposing that a single set of mechanisms is used 
    twice, once in planning action and once again in observing them.
  .notes: :t()
    What does this require?
  .slide
    p.em-above.notes.show -- in action observation, possible outcomes of observed actions are represented
  .slide
    p.em-above.notes.show -- these representations trigger planning as if performing actions directed to the outcomes
  .slide
    p.em-above.notes.show -- such planning generates predictions
    .notes predictions about joint displacements and their sensory conseuqences
  .slide
    p.em-above.notes.show -- a triggering representation is weakened if its predictions fail
  .notes: :t()
    The proposal is not specific to the idea of motor representations and processes,
    although there is good evidence for it (which I won't cover here because we're in Milan!)
    
+slide  
  .notes So here is my proposal about how pure goal ascription could work.
  ol.three-requirements(type='i', style="list-style-type: lower-roman")
    li reliably, R(a,G) 
      span.when when
      span  and 
      span.only-when only when
      span  a is directed to G
    li R(a,G) is readily detectable ...
    li ... without any knowledge of mental states
  p &nbsp;
  table
    tr.idea3
      td R(a,G) 
      td(style='padding-left:.5em;padding-right:.5em;') =
        sub df
      td  
        span a is the most justifiable action towards G available within the constraints of reality
    tr.idea4.hide
      td R
        sub M
        span (a,G) 
      td(style='padding-left:.5em;padding-right:.5em;') =
        sub df
      td  if M were tasked with producing G it would plan action a
  +line-through_('.idea3')
  +show('.idea4')
  .notes: :t()
    So here's the idea.
    The relation $R(a,G)$ should be defined relative to a planning mechanism.
    For planning mechanism $M$, $R{_M}(a,G)$ holds just if $M$ 
    were tasked with producing $G$ it would plan action $a$.
  +highlight-row_('.three-requirements li:eq(0)')
  +highlight-row('.three-requirements li:eq(1)')
  .notes: :t()
    With respect to the problems for the teleological stance, 
    which was about matching observer and agent, 
    we ensure a match insofar as observer an agent have similar planning mechanisms; 
    this means, of course, that they must have similar expertise.
  .notes: :t()
    We also ensure we get good  trade-offs---we get the right principle by deferring to the kinds 
    of planning mechanism responsible for producing the action.
  +highlight-row-remove_('.three-requirements li:eq(0)')
  +highlight-row-remove_('.three-requirements li:eq(1)')
  .slide
    .notes So I'm rejecting this claim.
    .notes Pure goal ascription need not involve reasoning at all.
    p &nbsp;
    p.handout.notes.show ‘when taking the teleological stance one-year-olds apply the same inferential principle of rational action that drives everyday mentalistic reasoning about intentional actions in adults’
    p.right.handout.notes.ctd.show (György Gergely and Csibra 2003; cf. Csibra, Bíró, et al. 2003; Csibra and Gergely 1998: 259)
    
    
+slide
  .notes Let me return to the two questions I started this section with:
  ol.notes.show
    li What model of action underpins  six- or twelve-month-old infants’ abilities to track the goals of actions? 
    li How could infants identify goals without ascribing intentions?
      p (I.e., How could pure goal ascription work?)
      p.step2.hide Answer: goal ascription is acting in reverse
  +highlight-row('li:eq(1)')
  .notes So far we've been discussing this question.
  .notes My answer is simple
  +show('.step2')
  .notes Answer: goal ascription is acting in reverse
  +highlight-row-remove_('li:eq(1)')
  +highlight-row('li:eq(0)')
  .notes: :t()
    But what about the first question?
    To describe how they identify goals (that is, distinguish among the actual and possible 
    outcomes of an action which its goals are) 
    is not yet quite to have explained how they model actions.
    Fortunately not much more is needed ...


+slide({bkg:'slide_budapest_lecture05_04.jpg'})
  .notes Recall that a model of action has to explain in virtue of what an action is directed to a goal.

+slide({bkg:'slide_budapest_lecture05_05.jpg'})
  .notes This, as I mentioned, is standardly done by invoking intentions.  But there is another way.

+slide
  .notes Recall this:
  table.notes.show
    tr
      td R
        sub M
        span (a,G) 
      td(style='padding-left:.5em;padding-right:.5em;') =
        sub df
      td  if M were tasked with producing G it would plan action a
  p.em-above
  .slide
    .notes Now do you remember Dennett's ingeneous twist in The Intentional Stance?  We're going to make the same move here.
    p.handout.notes.show ‘What it is to be a true believer is to be … a system whose behavior is reliably and voluminously predictable via the intentional strategy.’
    .handout.notes.ctd \citep[p.\ 15]{Dennett:1987sf}
    p.right Dennett 1987, p. 15
    .notes Dennett was interested in beliefs, but we can make essentially the same move for goals
    p.em-above
  .slide
    p.notes.show An outcome, G, is among the goals of an action, a, exactly if 
      span R
      sub M
      span (a,G)   
    .notes The idea is to turn a heuristic into a constitutive claim.
+slide({bkg:'unit_712/slide_intention_or_r.jpg'})  
  .notes: :t()
    So the infants' model of action is one on which goals are goals in virtue of relations
    between planning mechanisms and outcomes.
  .notes: :t()
    Note that this may not be a fully accurate model of action. 
    Just like impetus mechanics, it is useful even if only approximate.
    
  


